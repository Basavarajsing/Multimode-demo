<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>MultiMode â€” Multimodal Emotion Detection (Demo)</title>
<style>
  /* -------- Reset & base -------- */
  :root{
    --bg:#0f1724;
    --card:#0b1220;
    --accent:#00bcd4;
    --accent2:#ff6b6b;
    --glass: rgba(255,255,255,0.04);
  }
  *{box-sizing:border-box;margin:0;padding:0}
  html,body{height:100%}
  body{
    font-family: "Inter", system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial;
    background: linear-gradient(180deg,#071022 0%, #081428 60%), radial-gradient(circle at 10% 10%, rgba(0,188,212,0.03), transparent 10%);
    color:#e6eef6;
    -webkit-font-smoothing:antialiased;
    -moz-osx-font-smoothing:grayscale;
    line-height:1.4;
  }
  a{color:inherit;text-decoration:none}

  /* -------- Navbar -------- */
  .nav{
    display:flex;
    align-items:center;
    justify-content:space-between;
    gap:20px;
    padding:18px 28px;
    position:sticky; top:0;
    background:linear-gradient(90deg, rgba(8,18,36,0.7), rgba(6,10,20,0.45));
    border-bottom:1px solid rgba(255,255,255,0.03);
    backdrop-filter: blur(6px);
    z-index:200;
  }
  .brand{
    display:flex;align-items:center;gap:14px;font-weight:700;
  }
  .logo{
    width:44px;height:44px;border-radius:8px;background:linear-gradient(135deg,var(--accent),#2dd4bf);
    display:flex;align-items:center;justify-content:center;color:#021026;font-weight:800;font-size:18px;
    box-shadow: 0 6px 18px rgba(0,188,212,0.08), inset 0 -4px 12px rgba(0,0,0,0.12);
  }
  nav .links{display:flex;gap:18px;align-items:center}
  nav .links a{padding:8px 12px;border-radius:8px;font-weight:600;opacity:0.95}
  nav .links a:hover{background:rgba(255,255,255,0.03)}
  .cta{
    display:inline-flex;gap:10px;align-items:center;padding:8px 12px;border-radius:8px;background:linear-gradient(90deg,var(--accent2),#ff9b9b);
    color:#041018;font-weight:700;cursor:pointer;
  }

  /* Mobile menu */
  .menu-btn{display:none;background:transparent;border:0;color:inherit;font-size:18px;cursor:pointer}

  /* -------- Hero -------- */
  .hero{
    padding:48px 28px 8px;
    display:grid;
    grid-template-columns: 1fr 420px;
    gap:28px;
    align-items:start;
    max-width:1200px;margin:0 auto;
  }
  .hero .intro{
    background: linear-gradient(180deg, rgba(255,255,255,0.02), transparent);
    border-radius:14px;padding:26px;
    box-shadow: 0 8px 30px rgba(2,6,23,0.6);
    border:1px solid rgba(255,255,255,0.03);
  }
  .tag{display:inline-block;padding:6px 10px;border-radius:999px;background:rgba(0,188,212,0.08);color:var(--accent);font-weight:700;margin-bottom:12px}
  h1{font-size:30px;margin-bottom:8px}
  .lead{color:rgba(230,238,246,0.85);margin-bottom:14px}
  .meta{color:rgba(230,238,246,0.6);font-size:14px;margin-bottom:20px}

  /* -------- Quick stats / small cards -------- */
  .quick{
    display:flex;gap:12px;margin-top:12px;flex-wrap:wrap;
  }
  .stat{
    background:var(--card);padding:12px;border-radius:10px;min-width:120px;text-align:center;border:1px solid rgba(255,255,255,0.03)
  }
  .stat h3{font-size:16px;margin-bottom:6px}
  .stat p{font-size:13px;color:rgba(230,238,246,0.7)}

  /* -------- Right column demo card -------- */
  .demo-card{
    background:linear-gradient(180deg, rgba(255,255,255,0.02), transparent);
    border-radius:14px;padding:18px;border:1px solid rgba(255,255,255,0.03);
  }
  .demo-title{font-weight:700;margin-bottom:12px}
  .mode-pill{display:inline-block;padding:6px 10px;border-radius:999px;background:rgba(255,255,255,0.02);font-weight:700;color:var(--accent);margin-bottom:8px}

  /* -------- Marquee (120s) -------- */
  .marquee-wrap{margin:26px auto;max-width:1200px;padding:8px 20px;overflow:hidden;border-radius:10px;background:linear-gradient(90deg,#041428, #07162a);border:1px solid rgba(255,255,255,0.02)}
  .marquee-inner{display:inline-block;white-space:nowrap;animation:marquee 120s linear infinite}
  .marq-item{display:inline-block;padding:10px 28px;margin-right:18px;border-radius:8px;background:rgba(255,255,255,0.02);font-weight:700}

  @keyframes marquee{from{transform:translateX(0)} to{transform:translateX(-50%)}}

  /* -------- Dashboard / sections -------- */
  main{max-width:1200px;margin:18px auto;padding:0 18px 60px}
  section.panel{margin:18px 0;background:linear-gradient(180deg, rgba(255,255,255,0.02), transparent);border-radius:12px;padding:18px;border:1px solid rgba(255,255,255,0.03)}
  .grid-3{display:grid;grid-template-columns:repeat(auto-fit,minmax(240px,1fr));gap:16px}
  .card{background:var(--card);padding:16px;border-radius:10px;border:1px solid rgba(255,255,255,0.03)}
  .card h4{margin-bottom:8px}
  .muted{color:rgba(230,238,246,0.7);font-size:14px}

  /* -------- Demo area (image + audio) -------- */
  .io-row{display:flex;gap:12px;flex-wrap:wrap}
  .viewport{flex:1;min-width:260px;border-radius:10px;border:1px dashed rgba(255,255,255,0.03);padding:12px;background:linear-gradient(180deg, rgba(255,255,255,0.01), transparent)}
  #preview-img{max-width:100%;border-radius:8px;display:block;margin:8px 0}
  .controls{display:flex;gap:8px;flex-wrap:wrap;margin-top:8px}
  .btn{
    background:linear-gradient(90deg,var(--accent),#2dd4bf);
    color:#021026;padding:8px 12px;border-radius:8px;font-weight:700;border:none;cursor:pointer
  }
  .btn.alt{background:linear-gradient(90deg,var(--accent2),#ff9b9b);color:#071022}
  .small{font-size:13px;padding:8px 10px;border-radius:8px;border:1px solid rgba(255,255,255,0.03);background:transparent;color:inherit}

  /* -------- Results panel -------- */
  .result{
    display:flex;align-items:center;gap:12px;padding:12px;border-radius:8px;background:linear-gradient(90deg, rgba(255,255,255,0.02), transparent);
    border:1px solid rgba(255,255,255,0.03)
  }
  .emotion-badge{font-weight:800;padding:10px 14px;border-radius:999px;background:linear-gradient(90deg,#ffb86b,#ff6b6b);color:#021022}
  .confidence{font-weight:700}

  /* -------- Footer -------- */
  footer{
    margin-top:26px;padding:20px;background:linear-gradient(180deg,#041428,#06142a);text-align:center;border-top:1px solid rgba(255,255,255,0.02)
  }

  /* -------- Responsive -------- */
  @media (max-width:920px){
    .hero{grid-template-columns:1fr; padding:24px}
    .menu-btn{display:inline-block}
    nav .links{display:none}
  }
</style>
</head>
<body>

  <!-- NAV -->
  <header class="nav" role="banner">
    <div class="brand">
      <div class="logo">MM</div>
      <div>
        <div style="font-size:14px;opacity:0.9">MultiMode</div>
        <div style="font-size:12px;color:rgba(230,238,246,0.6)">Multimodal Emotion Detection â€” Demo</div>
      </div>
    </div>

    <nav>
      <div class="links" role="navigation">
        <a href="#home">Home</a>
        <a href="#demo">Demo</a>
        <a href="#objectives">Objectives</a>
        <a href="#team">Team</a>
        <a href="#apps">Applications</a>
      </div>
    </nav>

    <div style="display:flex;align-items:center;gap:12px">
      <div class="cta" onclick="document.getElementById('demo').scrollIntoView({behavior:'smooth'})">Try Demo</div>
      <button class="menu-btn" aria-label="menu" onclick="toggleMenu()">â˜°</button>
    </div>
  </header>

  <!-- HERO -->
  <section class="hero" id="home">
    <div class="intro">
      <span class="tag">Mini Project â€” A2 (2025-26)</span>
      <h1>Multimodal Human Emotion Detection</h1>
      <p class="lead">Combining facial and audio cues to robustly detect human emotions (visual fallback to audio when needed).</p>
      <p class="meta">Guide: Dr. H S Jayanna â€¢ Dept. of ISE â€¢ Siddaganga Institute of Technology</p>

      <div class="quick">
        <div class="stat">
          <h3>Batch</h3><p>2025-26 â€” A2</p>
        </div>
        <div class="stat">
          <h3>Project</h3><p>MultiMode â€” Demo Frontend</p>
        </div>
        <div class="stat">
          <h3>Modalities</h3><p>Face & Audio</p>
        </div>
      </div>
    </div>

    <aside class="demo-card">
      <div class="demo-title">Live Demo Panel</div>
      <div class="mode-pill" id="activeMode">Mode: â€”</div>
      <div class="muted" style="font-size:13px;margin-bottom:8px">This demo simulates detection on the client. Replace the JS stubs with ML models (TensorFlow.js / ONNX) for real detection.</div>
      <div style="display:flex;gap:8px;align-items:center;flex-wrap:wrap">
        <button class="btn" onclick="scrollToDemo()">Open Demo</button>
        <button class="btn alt" onclick="resetDemo()">Reset</button>
      </div>
    </aside>
  </section>

  <!-- MARQUEE -->
  <div class="marquee-wrap" aria-hidden="true">
    <div class="marquee-inner">
      <span class="marq-item">ðŸ“š Recommended Paper: "Audio-Visual Emotion Recognition (2023)"</span>
      <span class="marq-item">ðŸ”¬ Try: integrate TensorFlow.js face-api or MediaPipe</span>
      <span class="marq-item">ðŸŽ¤ Tip: Use WebAudio RMS for vocal energy features</span>
      <span class="marq-item">ðŸ’¡ Demo built: Frontend-only â€” plug ML later</span>
      <span class="marq-item">ðŸ§ª Test Cases: low-light, occlusion, noisy audio</span>
      <span class="marq-item">ðŸ“š Recommended Paper: "Audio-Visual Emotion Recognition (2023)"</span>
    </div>
  </div>

  <!-- MAIN -->
  <main>
    <!-- DEMO: upload image / audio -->
    <section class="panel" id="demo" aria-labelledby="demoHeading">
      <h2 id="demoHeading">Interactive Demo â€” Visual â‡„ Audio</h2>
      <p class="muted">Upload an image (face) or record/upload audio. The interface chooses the best modality and shows a simulated emotion.</p>

      <div class="grid-3" style="margin-top:12px">
        <div class="card">
          <h4>Visual Input</h4>
          <div class="io-row">
            <div class="viewport">
              <input type="file" id="imgInput" accept="image/*" class="small">
              <img id="preview-img" alt="preview" style="display:none">
              <canvas id="imgCanvas" style="display:none"></canvas>
              <div class="muted" style="font-size:13px">Upload a clear face photo for best simulated results.</div>
            </div>
          </div>
        </div>

        <div class="card">
          <h4>Audio Input</h4>
          <div class="io-row">
            <div class="viewport">
              <div style="display:flex;gap:8px;align-items:center">
                <button class="btn" id="recordBtn">Record (Web)</button>
                <button class="small" id="stopBtn" disabled>Stop</button>
                <input type="file" id="audioFile" accept="audio/*" class="small">
              </div>
              <div id="audioList" style="margin-top:12px"></div>
              <div class="muted" style="font-size:13px;margin-top:8px">You can record with your mic or upload an audio clip (speech).</div>
            </div>
          </div>
        </div>

        <div class="card">
          <h4>Detection Result</h4>
          <div id="resultArea" style="margin-top:8px">
            <div class="result" id="resultCard">
              <div style="flex:1">
                <div style="font-size:13px;color:rgba(230,238,246,0.7)">Chosen Modality</div>
                <div id="chosenMod" style="font-weight:800;font-size:18px;margin-top:6px">â€”</div>
              </div>
              <div style="text-align:right">
                <div class="emotion-badge" id="emotionLabel">â€”</div>
                <div style="margin-top:8px;font-size:13px">Confidence: <span class="confidence" id="confidence">â€”</span></div>
              </div>
            </div>

            <div style="margin-top:12px">
              <div class="muted">Notes:</div>
              <ul style="margin-left:14px;margin-top:8px;color:rgba(230,238,246,0.75)">
                <li>Visual check: measures image brightness & variance to detect presence.</li>
                <li>Audio check: analyses loudness & pitch energy as simple cues.</li>
                <li>This is a demo frontend. Replace `analyzeImage()` and `analyzeAudio()` with real models for production.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Objectives -->
    <section class="panel" id="objectives">
      <h2>Objectives</h2>
      <div style="margin-top:10px" class="grid-3">
        <div class="card">
          <h4>1. Multimodal System</h4>
          <p class="muted">Combine facial and speech signals to determine emotions in real-time.</p>
        </div>
        <div class="card">
          <h4>2. Robust Face Analysis</h4>
          <p class="muted">Implement robust facial emotion analysis for normal lighting conditions.</p>
        </div>
        <div class="card">
          <h4>3. Speech-based Recognition</h4>
          <p class="muted">Extract vocal features (pitch, tone) to detect emotions when video is unreliable.</p>
        </div>
        <div class="card">
          <h4>4. Dynamic Switching</h4>
          <p class="muted">Switch modalities automatically based on input quality & availability.</p>
        </div>
        <div class="card">
          <h4>5. Fusion & Validation</h4>
          <p class="muted">Fuse features from both modalities and validate in varied real-world scenarios.</p>
        </div>
      </div>
    </section>

    <!-- Team -->
    <section class="panel" id="team">
      <h2>Team</h2>
      <div style="margin-top:12px" class="grid-3">
        <div class="card">
          <h4>BASAVARAJSING</h4>
          <p class="muted">USN: 1SI23IS010</p>
        </div>
        <div class="card">
          <h4>KIRAN H S</h4>
          <p class="muted">USN: 1SI23IS035</p>
        </div>
        <div class="card">
          <h4>NIKHIL K G</h4>
          <p class="muted">USN: 1SI23IS054</p>
        </div>
        <div class="card">
          <h4>NAGARJUN K</h4>
          <p class="muted">USN: 1SI23IS051</p>
        </div>
        <div class="card">
          <h4>Guide</h4>
          <p class="muted">Dr. H S Jayanna â€” Professor, ISE</p>
        </div>
      </div>
    </section>

    <!-- Applications -->
    <section class="panel" id="apps">
      <h2>Applications</h2>
      <div style="margin-top:10px" class="grid-3">
        <div class="card"><h4>Surveillance & Security</h4><p class="muted">Detect suspicious/aggresive behavior with audio/visual cues.</p></div>
        <div class="card"><h4>Healthcare & Mental Health</h4><p class="muted">Remote monitoring for emotional distress.</p></div>
        <div class="card"><h4>HCI</h4><p class="muted">Emotion-aware interfaces & games.</p></div>
        <div class="card"><h4>Assistive Tech</h4><p class="muted">Help caregivers interpret emotions for non-verbal individuals.</p></div>
        <div class="card"><h4>Education</h4><p class="muted">Monitor engagement & provide adaptive feedback.</p></div>
        <div class="card"><h4>Customer Experience</h4><p class="muted">Analyze customer emotions during interactions.</p></div>
      </div>
    </section>
  </main>

  <!-- FOOTER -->
  <footer>
    <div style="max-width:900px;margin:0 auto">
      <div style="display:flex;justify-content:space-between;gap:12px;flex-wrap:wrap">
        <div>
          <strong>MultiMode</strong><div class="muted" style="margin-top:4px">Multimodal Emotion Detection â€” Demo Frontend</div>
        </div>
        <div class="muted" style="text-align:right">Siddaganga Institute of Technology - Tumakuru â€¢ Dept. of ISE â€¢ Â© 2025</div>
      </div>
    </div>
  </footer>

<script>
/*
  MultiMode demo front-end logic
  - Visual: reads uploaded image to canvas, computes mean luminance & variance to estimate presence/visibility.
  - Audio: records audio or accepts file, computes RMS (energy) and average frequency estimate to map to basic emotions.
  - Chosen modality logic: prefer Visual when image is "valid", else use Audio if available. If both, show both and choose higher confidence.
  - IMPORTANT: This is a simulated frontend: replace analyzeImage()/analyzeAudio() with real ML pipeline when integrating models.
*/

/* ---------- helpers ---------- */
const el = id => document.getElementById(id);
const setText = (id, txt) => { const d = el(id); if(d) d.textContent = txt; }

/* ---------- UI & navigation ---------- */
function scrollToDemo(){ document.getElementById('demo').scrollIntoView({behavior:'smooth'}) }
function resetDemo(){
  el('imgInput').value = '';
  el('preview-img').style.display='none';
  el('audioList').innerHTML='';
  setMode('â€”');
  setResult('â€”','â€”', 'â€”');
}

/* ---------- menu (mobile) ---------- */
function toggleMenu(){
  const nav = document.querySelector('.links');
  if(nav.style.display === 'flex') nav.style.display='';
  else nav.style.display='flex';
}

/* ---------- image handling ---------- */
const imgInput = el('imgInput');
const previewImg = el('preview-img');
const canvas = el('imgCanvas');

imgInput.addEventListener('change', async (e)=>{
  const f = e.target.files && e.target.files[0];
  if(!f) return;
  const url = URL.createObjectURL(f);
  previewImg.src = url;
  previewImg.style.display = 'block';
  await new Promise(r => previewImg.onload = r);
  // draw to canvas and analyze
  canvas.width = previewImg.naturalWidth;
  canvas.height = previewImg.naturalHeight;
  const ctx = canvas.getContext('2d');
  ctx.drawImage(previewImg, 0, 0);
  const result = analyzeImage(ctx, canvas.width, canvas.height);
  handleDetection({visual: result});
});

/* analyzeImage: compute luminance mean and variance as proxy for "face visible" */
function analyzeImage(ctx, w, h){
  // sample pixels (downscale large images)
  const sampleStep = Math.max(1, Math.floor(Math.max(w,h)/200));
  const data = ctx.getImageData(0,0,w,h).data;
  let sum=0, sumSq=0, count=0;
  for(let y=0;y<h;y+=sampleStep){
    for(let x=0;x<w;x+=sampleStep){
      const i = (y*w + x) * 4;
      const r = data[i], g = data[i+1], b = data[i+2];
      // luminance
      const L = 0.2126*r + 0.7152*g + 0.0722*b;
      sum += L; sumSq += L*L; count++;
    }
  }
  const mean = sum/count;
  const variance = (sumSq/count) - (mean*mean);
  // heuristics:
  const facePresentScore = Math.max(0, Math.min(1, (variance/2000) + ((mean>40 && mean<220)?0.2:0)));
  // simulated emotion mapping by brightness/variance (just demo)
  let emotion = 'Neutral';
  if(mean > 180) emotion = 'Happy';
  else if(mean < 40) emotion = 'Sad';
  else if(variance > 1200) emotion = 'Surprised';
  const confidence = Math.round(facePresentScore*100);
  return {mode:'visual', faceScore: facePresentScore, emotion, confidence};
}

/* ---------- Audio handling (record + file) ---------- */
let mediaRecorder, audioChunks = [];
const recordBtn = el('recordBtn'), stopBtn = el('stopBtn'), audioList = el('audioList');

recordBtn.addEventListener('click', async ()=>{
  if(!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia){
    alert('Microphone not supported in this browser.');
    return;
  }
  try{
    const stream = await navigator.mediaDevices.getUserMedia({audio:true});
    mediaRecorder = new MediaRecorder(stream);
    audioChunks = [];
    mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
    mediaRecorder.onstop = async ()=>{
      const blob = new Blob(audioChunks, {type:'audio/webm'});
      handleAudioBlob(blob);
      stream.getTracks().forEach(t=>t.stop());
    };
    mediaRecorder.start();
    recordBtn.disabled = true; stopBtn.disabled = false;
    recordBtn.textContent = 'Recording...';
  }catch(err){ console.error(err); alert('Permission denied / mic error'); }
});

stopBtn.addEventListener('click', ()=>{
  if(mediaRecorder && mediaRecorder.state !== 'inactive'){
    mediaRecorder.stop();
    recordBtn.disabled = false; stopBtn.disabled = true;
    recordBtn.textContent = 'Record (Web)';
  }
});

el('audioFile').addEventListener('change', (e)=>{
  const f = e.target.files && e.target.files[0];
  if(!f) return;
  handleAudioBlob(f);
});

function handleAudioBlob(blob){
  const url = URL.createObjectURL(blob);
  audioList.innerHTML = `<audio controls src="${url}"></audio>`;
  // analyze via offline audio context
  analyzeAudio(blob).then(res => handleDetection({audio:res}));
}

/* analyzeAudio: compute RMS (energy) and rough avg freq via FFT (basic) */
async function analyzeAudio(blob){
  try{
    const arrayBuffer = await blob.arrayBuffer();
    const ac = new (window.OfflineAudioContext || window.webkitOfflineAudioContext)(1, 22050*3, 22050);
    const decoded = await ac.decodeAudioData(arrayBuffer);
    // mix to mono
    const ch = decoded.getChannelData(0);
    // compute RMS and simple centroid-ish estimate
    let sum = 0, count=0;
    for(let i=0;i<ch.length;i+=Math.max(1,Math.floor(ch.length/22050))){
      sum += Math.abs(ch[i]); count++;
    }
    const meanAbs = sum/count;
    // energy proxy
    const energy = Math.min(1, meanAbs*5); // normalized
    // map energy to emotions (very rough)
    let emotion = 'Neutral';
    if(energy > 0.25 && energy < 0.5) emotion = 'Happy';
    else if(energy >= 0.5) emotion = 'Angry';
    else if(energy < 0.08) emotion = 'Sad';
    const confidence = Math.round(energy*100);
    return {mode:'audio', energy, emotion, confidence};
  }catch(err){
    console.error('Audio analysis error',err);
    return {mode:'audio', energy:0, emotion:'Neutral', confidence:20};
  }
}

/* ---------- modality decision & result UI ---------- */
function setMode(text){ el('activeMode').textContent = 'Mode: ' + text }
function setResult(modal, emotion, conf){
  setText('chosenMod', modal || 'â€”');
  setText('emotionLabel', emotion || 'â€”');
  setText('confidence', (conf===undefined||conf===null)?'â€”': conf + '%');
}

/* handleDetection accepts object: {visual: {...}} and/or {audio: {...}} */
function handleDetection({visual, audio}){
  // choose modality:
  let chosen = null;
  if(visual && visual.faceScore > 0.25 && audio){
    // both available: choose higher confidence
    chosen = (visual.confidence >= audio.confidence) ? {...visual, chosenBy:'visual'} : {...audio, chosenBy:'audio'};
  } else if(visual && visual.faceScore > 0.25){
    chosen = {...visual, chosenBy:'visual'};
  } else if(audio){
    chosen = {...audio, chosenBy:'audio'};
  } else if(visual){
    // visual exists but low score -> fallback to audio if available else still show visual
    chosen = {...visual, chosenBy:'visual'};
  }
  if(!chosen){
    setMode('None');
    setResult('â€”','â€”','â€”');
    return;
  }
  // Update UI
  const modeName = chosen.mode === 'visual' ? 'Visual (Face)' : 'Audio (Voice)';
  setMode(modeName);
  // present emotion + confidence
  setResult(modeName, chosen.emotion || 'Neutral', Math.max(10, chosen.confidence || 40));

  // small animated highlight
  const card = el('resultCard');
  card.style.transform = 'translateY(-6px)';
  setTimeout(()=>card.style.transform='translateY(0)',450);
}

/* ---------- initial state ---------- */
resetDemo();
setMode('Idle');

/* ---------- accessibility: keyboard nav (small) ---------- */
window.addEventListener('keydown', (e)=>{
  if(e.key === 'd'){ document.getElementById('demo').scrollIntoView({behavior:'smooth'}) }
});
</script>
</body>
</html>
